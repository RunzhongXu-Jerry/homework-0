{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "最近在赶期末的作业，只好先埋坑了。\n",
    "1.特征获取\n",
    "特征工程加好的算法是基于规则的一种更好表现形式。一般在编写程序的时候都是先找到影响最终结果的一些规则，再使用if else 或者switch 等决定程序的逻辑结构。当特征数到达几十的时候，此类程序可能已经不能维护了，而特征工程就是将这些规则以一列数字的形式表现出来，再通过算法计算得到此列对最终结果的影响，最终得到一个属性向量和一个特征向量，再使两个向量相乘得到最终结果。就比如三个特征 ，访问量、距离发布天数、是否精华，物品A为（3000 ， 3 ，1）物品B为（2000 ， 2 ， 0）通过算法得到特征权值为（0.001 ， -1 ，2）所以A物品的推荐度为2 ，B物品的推荐度为0，所有推荐A物品。好的特征工程就是能够最好的表述出能够对结果影响的因素，此因素可能是属性值，也可能是统计值。\n",
    "\n",
    "不同的推荐场景所使用的特征也是不同的，比如新闻推荐领域最重要的特征是发布时间、热度、是否精华。外卖推荐领域为位置、评价、是否品牌、活动如何。书籍推荐领域为喜好领域的权威书籍。电影推荐领域为评分、热度、演员、类型等。这部分工作需要很多领域知识，一般需要一组的研究人员讨论，要认真的思考这个特定问题有些什么和别的问题不同的特征，也建议和市场部销售部等有领域知识的专家讨论。经验上来说，这些特征提取的越多越好，并不用担心特征过多，因为推荐系统的数据量都比较大，并且基于一些规则可以很好的筛选特征。\n",
    "\n",
    "2.特征处理\n",
    "2.1特征清洗\n",
    "\n",
    "特征处理第一步为特征清洗，比如清洗掉爬虫数据。机器学习模型要求正负样本数量相差不多，而推荐场景中用户访问过的毕竟是极少的几个，所以还要对负样本进行采样。\n",
    "\n",
    "2.2数据预处理\n",
    "\n",
    "通过特征提取，我们能得到未经处理的特征，这时的特征可能有以下问题：\n",
    "\n",
    "不属于同一量纲：即特征的规格不一样，不能够放在一起比较。无量纲化可以解决这一问题。\n",
    "信息冗余：对于某些定量特征，其包含的有效信息为区间划分，例如学习成绩，假若只关心“及格”或不“及格”，那么需要将定量的考分，转换成“1”和“0”表示及格和未及格。二值化可以解决这一问题。\n",
    "定性特征不能直接使用：某些机器学习算法和模型只能接受定量特征的输入，那么需要将定性特征转换为定量特征。最简单的方式是为每一种定性值指定一个定量值，但是这种方式过于灵活，增加了调参的工作。通常使用哑编码的方式将定性特征转换为定量特征：假设有N种定性值，则将这一个特征扩展为N种特征，当原始特征值为第i种定性值时，第i个扩展特征赋值为1，其他扩展特征赋值为0。哑编码的方式相比直接指定的方式，不用增加调参的工作，对于线性模型来说，使用哑编码后的特征可达到非线性的效果。\n",
    "存在缺失值：缺失值需要补充。\n",
    "信息利用率低：不同的机器学习算法和模型对数据中信息的利用是不同的，之前提到在线性模型中，使用对定性特征哑编码可以达到非线性的效果。类似地，对定量变量多项式化，或者进行其他的转换，都能达到非线性的效果。\n",
    "2.2.1 无量纲化\n",
    "\n",
    "无量纲化使不同规格的数据转换到同一规格。常见的无量纲化方法有标准化和区间缩放法。标准化的前提是特征值服从正态分布，标准化后，其转换成标准正态分布。区间缩放法利用了边界值信息，将特征的取值区间缩放到某个特点的范围，例如[0, 1]等。\n",
    "\n",
    "\n",
    "2.2.2 对特定特征哑编码\n",
    "\n",
    "在研究一个因变量的时候，解释变量中除了定量变量，有时候会有一些定性变量，比如性别、年龄、宗教、民族、婚姻状况、教育程度等。这些定性变量也可以成为指标变量、二元变量或分类变量。此时需要使用虚拟变量。 引入哑变量可使线形回归模型变得更复杂，但对问题描述更简明，一个方程能达到俩个方程的作用，而且接近现实。 如果某个因素有n种选择，则将其用哑变量引入模型时，要设置n-1个哑变量，以避免完全的多重共线性。如性别的选择有两种，则引入一个哑变量，是男则数值为1，否则为0，当然也可以设置为女为1，否则为0。季节的选择有4个，则引入3个哑变量，哑变量1：春为1，否则为0.哑变量2：夏为1，否则为0.哑变量3：秋为1，否则为0.\n",
    "\n",
    "2.2.3 特征选择\n",
    "\n",
    "数据预处理完后第一步为分析数据，对数据整体结构有一个大致的把握，这里我们将数据分为：类别型、连续型、日期型、单词型、文本型。每种类型数据有特定的分析方式，如类别型为输出饼图，连续型为输出直方图、KDE图、CDF图，单词型为输出词频，类别型和类别型做频繁项分析，连续型和连续型做回归分析，类别型和连续型输出不同类别的直方图等等。\n",
    "\n",
    "当对数据有一个大致理解后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。通常来说，从两个方面考虑来选择特征：\n",
    "\n",
    "特征是否发散：如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。\n",
    "特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优选选择。\n",
    "　　根据特征选择的形式又可以将特征选择方法分为3种：\n",
    "\n",
    "Filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。\n",
    "Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。\n",
    "Embedded：集成法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。\n",
    "2.2.4 降维\n",
    "\n",
    "当特征选择完成后，可以直接训练模型了，但是可能由于特征矩阵过大，导致计算量大，训练时间长的问题，因此降低特征矩阵维度也是必不可少的。常见的降维方法除了以上提到的基于L1惩罚项的模型以外，另外还有主成分分析法（PCA）和线性判别分析（LDA），线性判别分析本身也是一个分类模型。PCA和LDA有很多的相似点，其本质是要将原始的样本映射到维度更低的样本空间中，但是PCA和LDA的映射目标不一样：PCA是为了让映射后的样本具有最大的发散性；而LDA是为了让映射后的样本有最好的分类性能。所以说PCA是一种无监督的降维方法，而LDA是一种有监督的降维方法。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
